





pip install pytorch-lightning pytorch-forecasting pandas numpy matplotlib optuna optuna-integration[pytorch_lightning] pytorch_optimizer





import pandas as pd

data_path = "DATA_PATH"
merged_df = pd.read_csv(data_path)
print(len(merged_df))
merged_df.columns


cols_to_convert = ['month', 'day', 'day_of_week']
merged_df[cols_to_convert] = merged_df[cols_to_convert].astype(str)
merged_df = merged_df[['Date', 'Open', 'High', 'Low', 'Close',
       'Volume', 'Dividends', 'Stock Splits', 'month', 'day', 'day_of_week',
       'NASDAQ', 'SNP', 'DJI', 'RUT', 'VIX', 'XLK', 'XLE', 'XLF', 'XLV', 'RSI',
       'MA_20', 'MA_50', 'MA_200', 'log_return', 'RV_20', 'RV_50', 'symbol',
       'time_idx', 'sentiment']]


merged_df.head()


# Ensure non NaN in input data
null_counts = merged_df.isna().sum()

# Display the result
print(null_counts)


# Ensure each stock has its own consecutive time_idx
# merged_df["time_idx"] = merged_df.groupby("symbol")["Date"].rank(method="dense").astype(int) - 1





!rm -r /kaggle/working/optuna_test/*


!rm -r /kaggle/working/lightning_logs/*


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import optuna
from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer
from pytorch_forecasting.data import GroupNormalizer
from pytorch_forecasting.metrics import QuantileLoss, MAE
from lightning.pytorch import Trainer, Callback
import gc
import pickle
from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters



# Define the custom callback
class GarbageCollectionCallback(Callback):
    def on_epoch_end(self, trainer: Trainer, pl_module):
        """
        This method is called at the end of each epoch. 
        It forces garbage collection to free up unused memory.
        """
        print(f"Epoch {trainer.current_epoch} ended. Running garbage collection...")
        gc.collect()  # Manually trigger garbage collection
        torch.cuda.empty_cache()  # Clear GPU cache
        print("Garbage collection triggered.")
      
    def on_train_end(self, trainer, pl_module):
        """
        This method is called once, after the entire training process is finished.
        """
        del trainer  # Delete the trainer object
        gc.collect()  # Manually trigger garbage collection
        print("Training has ended.")


# 1. Define the training dataset
max_prediction_length = 1   # Predict next 5 days
max_encoder_length = 10     # Use past 20 days for training

# training_cutoff = merged_df["time_idx"].max() - max_prediction_length
cutoff_date = '2025-02-10'
training_cutoff = 2320
print(cutoff_date)
train_data = merged_df[merged_df["Date"] <= cutoff_date]
print(len(train_data))
val_data = merged_df[merged_df["Date"] > cutoff_date]
print(len(val_data))

# Convert to PyTorch Forecasting Dataset
training = TimeSeriesDataSet(
    train_data,
    time_idx="time_idx",
    target="Close",
    group_ids=["symbol"],  
    max_encoder_length=max_encoder_length,
    max_prediction_length=max_prediction_length,
    static_categoricals=["symbol"],
    time_varying_known_categoricals=['month', 'day', 'day_of_week'],
    time_varying_known_reals=["time_idx"],
    time_varying_unknown_reals=["Close", "Open", "High", "Low", "Volume", "RSI", "sentiment", "MA_20", "MA_50", "MA_200", "log_return", "RV_20", "RV_50", 'NASDAQ', 'SNP', 'DJI', 'RUT', 'VIX', 'XLK', 'XLE', 'XLF', 'XLV', 'RSI'],
    target_normalizer=GroupNormalizer(groups=["symbol"]),
    allow_missing_timesteps=True
)

validation = TimeSeriesDataSet.from_dataset(training, val_data, predict=True)

# Create DataLoaders
batch_size = 32
train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=3)
val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=3)


train_data.groupby("symbol")["Date"].count()


# Create hyper param tuning 
# loss_fn = QuantileLoss(quantiles=[0.5])
loss_fn = MAE()
study = optimize_hyperparameters(
    train_dataloader,
    val_dataloader,
    model_path="optuna_test",
    n_trials=20,
    max_epochs=10,
    gradient_clip_val_range=(0.1, 5.0),
    hidden_size_range=(100,128),
    hidden_continuous_size_range=(100,128),
    attention_head_size_range=(3, 4),
    learning_rate_range=(0.000001, 0.1),
    dropout_range=(0.1, 0.5),
    trainer_kwargs=dict(limit_train_batches=500, accelerator="auto"),
    reduce_on_plateau_patience=4,
    optimizer='adamw',
    loss=loss_fn,
    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder
)


print(study.best_trial.params)


learning_rate=study.best_trial.params['learning_rate']
hidden_size=study.best_trial.params['hidden_size']
attention_head_size=study.best_trial.params['attention_head_size']
dropout=study.best_trial.params['dropout']
hidden_continuous_size=study.best_trial.params['hidden_continuous_size']
gradient_clip_val=study.best_trial.params['gradient_clip_val']


# model_path = "/kaggle/input/tft_model0/pytorch/default/1/tft_model-epoch31-val_loss40.1897.ckpt"
# best_tft = model # TemporalFusionTransformer.load_from_checkpoint(best_model_path)





from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
from lightning.pytorch import Trainer

# Define the checkpoint callback to save the model
checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",  # Directory to save checkpoints
    filename="tft_model-{epoch:02d}-{val_loss:.4f}",  # Save the model with epoch and validation loss
    monitor="val_loss",  # Monitor the validation loss for saving the best model
    mode="min",  # Save the model with the lowest validation loss
    save_top_k=1  # Save only the best model (change to save more if needed)
)

# Create the EarlyStopping callback
early_stopping = EarlyStopping(
    monitor='val_loss',  # Metric to monitor (can also be 'val_accuracy', etc.)
    patience=8,          # Number of epochs with no improvement before stopping
    verbose=True,        # Print messages when stopping
    mode='min',          # 'min' for loss (lower is better), 'max' for accuracy (higher is better)
)


trainer = Trainer(
    max_epochs=50,
    accelerator="auto",
    enable_model_summary=True,
    gradient_clip_val=gradient_clip_val,
    callbacks=[checkpoint_callback, early_stopping],
    # limit_train_batches=50,  # coment in for training, running valiation every 30 batches
    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs
)

loss_fn = QuantileLoss(quantiles=[0.5])

tft = TemporalFusionTransformer.from_dataset(
    training,
    learning_rate=learning_rate,
    hidden_size=hidden_size,
    attention_head_size=attention_head_size,
    dropout=dropout,
    hidden_continuous_size=hidden_continuous_size,
    loss=MAE(),
    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches
    optimizer="adamw",
    reduce_on_plateau_patience=3,
)


# trainer = Trainer()
# trainer.fit(model, ckpt_path="/kaggle/input/tft_q3_proto/pytorch/default/1/tft_model-epoch14-val_loss45.6763.ckpt")


trainer.fit(
    tft,
    train_dataloaders=train_dataloader,
    val_dataloaders=val_dataloader,
)


print("Training Complete") 








# # load the best model according to the validation loss
# # (given that we use early stopping, this is not necessarily the last epoch)
# best_model_path = "/kaggle/input/tft_q3_proto/pytorch/default/1/tft_model-epoch14-val_loss45.6763.ckpt"
# best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)


# # raw predictions are a dictionary from which all kind of information including quantiles can be extracted
# raw_predictions = best_tft.predict(
#     val_dataloader, mode="raw", return_x=True, trainer_kwargs=dict(accelerator="auto")
# )

# quantile_predictions = best_tft.predict(val_dataloader, mode="quantiles")


# import matplotlib.pyplot as plt
# # for idx in range(10):  # plot 10 examples
# #     best_tft.plot_prediction(
# #         raw_predictions.x,
# #         raw_predictions.output,
# #         idx=idx,
# #         show_future_observed=True,
# #     )
# quantile_predictions.shape

# median_forecast = quantile_predictions[:, :, 3]  # Extract median (q=0.5)
# low_forecast = quantile_predictions[:, :, 0]     # Extract q=0.05 (pessimistic case)
# high_forecast = quantile_predictions[:, :, 6] 

# # Plotting
# plt.plot(median_forecast, label="Median Forecast (50%)", color="blue", linewidth=2)
# plt.plot(low_forecast, label="Lower Bound (5%)", color="red", linestyle="dashed")
# plt.plot(high_forecast, label="Upper Bound (95%)", color="green", linestyle="dashed")

# # Labels and title
# plt.xlabel("Time Steps")
# plt.ylabel("Predicted Value")
# plt.title("TFT Forecast with 5%, 50%, and 95% Quantiles")
# plt.legend()
# plt.grid(True)
# plt.show()







# symbol = 'TSLA'
# windows = 2000


# stock_df = merged_df[merged_df['symbol'] == symbol]
# predict_df = stock_df.tail(windows).reset_index()
# pred_input_df = pd.concat([predict_df, predict_df.tail(1)], ignore_index=True)
# pred_input_df['time_idx'] = range(len(pred_input_df))
# predictions = TimeSeriesDataSet.from_dataset(training, pred_input_df)

# pred_dataloader = predictions.to_dataloader(train=False, batch_size=batch_size)

# predictions_out = best_tft.predict(
#     pred_dataloader, mode="raw", return_x=True, trainer_kwargs=dict(accelerator="cpu")
# )

# predictions_q = best_tft.predict(pred_dataloader)


# median_forecast = predictions_q[:, 0]  # Extract median (q=0.5)
# low_forecast = predictions_q[:, 0, 0]     # Extract q=0.05 (pessimistic case)
# high_forecast = predictions_q[:, 0, 2] 


# print(predict_df['Date'][-10:])
# print(median_forecast[-10:].tolist())
# print(predict_df['Close'][-10:])



# # # Plotting
# pred_size = 40
# plt.plot(predict_df['Date'][-pred_size-1:].to_list() + ['2025-03-15'], median_forecast.cpu()[-pred_size-2:], label="Predicted", color="blue", linewidth=2, linestyle="dashed")
# #plt.plot(low_forecast.cpu(), label="Lower Bound (5%)", color="red", linestyle="dashed")
# #plt.plot(high_forecast.cpu(), label="Upper Bound (95%)", color="green", linestyle="dashed")
# plt.plot(predict_df['Date'][-pred_size-1:], predict_df['Close'][-pred_size-1:], label="Actual", color="black")

# # # Labels and title
# plt.xlabel("Time Steps")
# plt.ylabel("Predicted Value")
# plt.title(f"{symbol} Forecast")
# plt.legend()
# plt.grid(True)
# plt.show()


# predict_df['Close'][-pred_size:]


# for idx in range(10):  # plot 10 examples
#     best_tft.plot_prediction(
#         predictions.x,
#         predictions.output,
#         idx=idx,
#         show_future_observed=True,
#     )
